{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the coffee data CSV file into a Spark DataFrame.\n",
    "# All the columns should be floats except for the 'Date' and 'Currency' columns.\n",
    "# Columns from Aggregate Functions\n",
    "\n",
    "data_file = './data/coffee.csv'\n",
    "\n",
    "csvSchema = StructType([\n",
    "    StructField('Date',DateType(),True),\n",
    "    StructField('Open',FloatType(),True),\n",
    "    StructField('High',FloatType(),True),\n",
    "    StructField('Low',FloatType(),True),\n",
    "    StructField('Close',FloatType(),True),\n",
    "    StructField('Volume',FloatType(),True),\n",
    "    StructField('Currency',StringType(),True),\n",
    "    ])\n",
    "\n",
    "df=spark.read.format(\"csv\").option('header','true').schema(csvSchema).load(data_file)\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the DataFrame where the values are the difference between 'Open' and 'Close'.\n",
    "\n",
    "# Add a column to the DataFrame where the values are the difference between 'High' and 'Low'.\n",
    "\n",
    "# Add a column to the DataFrame where the values are 'True' if the volume for that day was 100 or above, and otherwise 'False'.\n",
    "\n",
    "# Once you have a column for the difference between 'Open' and 'Close', add another column that contains the absolute values of the numbers in that column.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ee0ce4fd339fd4a2a0b7727545bfd097941a5f705c1273b8bf2d33c3e65523"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
